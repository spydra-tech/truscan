rules:
  # ============================================================================
  # LLM02: Command Injection - Cohere Generic
  # ============================================================================
  # Detects LLM output used in command execution functions
  # OWASP: LLM02
  # CWE: CWE-78
  # ============================================================================

  - id: cohere-command-injection-subprocess
    mode: taint
    pattern-sources:
      - pattern: |
          cohere.Client().chat(...)
      - pattern: |
          cohere.ClientV2().chat(...)
      - pattern: |
          $CLIENT.chat(...)
      - pattern: |
          cohere.Client().generate(...)
      - pattern: |
          $CLIENT.generate(...)
    pattern-sinks:
      - pattern: |
          subprocess.run($COHERE_OUTPUT, ...)
      - pattern: |
          subprocess.call($COHERE_OUTPUT, ...)
      - pattern: |
          subprocess.Popen($COHERE_OUTPUT, ...)
      - pattern: |
          os.system($COHERE_OUTPUT)
      - pattern: |
          os.popen($COHERE_OUTPUT)
    message: "LLM02: Command Injection - Cohere output passed to command execution functions"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: command-injection
      owasp: "LLM02"
      owasp-title: "Insecure Output Handling"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM02.html"
      cwe: ["CWE-78"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/78.html"]
      tags: ["owasp", "llm02", "insecure-output", "command-injection", "subprocess", "os.system", "llm", "ai-security", "cohere", "taint", "ai-enhanced"]
      technology: ["python", "cohere", "llm"]
      confidence: "high"
      impact: "critical"
      likelihood: "high"
      description: "Cohere chat or generate output flows into command execution functions (subprocess, os.system). If the LLM output contains commands or command-like text, this can lead to arbitrary command execution."
      remediation: "Never execute LLM output as commands. Use safe parsing, validation, and structured output formats. If command execution is needed, use allowlists, parameterized commands, or sandboxed execution environments."
      ai_analysis_recommended: true
      examples:
        vulnerable: "samples/cohere_command_injection.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://cwe.mitre.org/data/definitions/78.html"
    paths:
      include:
        - "*.py"

  - id: cohere-command-injection-direct
    patterns:
      - pattern-either:
          # Chat response to subprocess/os.system
          - pattern: |
              $RESPONSE = cohere.Client().chat(...)
              ...
              $VAR = $RESPONSE.text
              ...
              subprocess.run($VAR, ...)
          - pattern: |
              $RESPONSE = cohere.Client().chat(...)
              ...
              $VAR = $RESPONSE.message.content
              ...
              subprocess.run($VAR, ...)
          - pattern: |
              $RESPONSE = $CLIENT.chat(...)
              ...
              $VAR = $RESPONSE.text
              ...
              os.system($VAR)
          # Generate response to subprocess/os.system
          - pattern: |
              $RESPONSE = cohere.Client().generate(...)
              ...
              $VAR = $RESPONSE.generations[0].text
              ...
              subprocess.run($VAR, ...)
          - pattern: |
              $RESPONSE = $CLIENT.generate(...)
              ...
              $VAR = $RESPONSE.generations[0].text
              ...
              os.system($VAR)
    message: "LLM02: Command Injection - Cohere output extracted and passed to command execution functions"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: command-injection
      owasp: "LLM02"
      owasp-title: "Insecure Output Handling"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM02.html"
      cwe: ["CWE-78"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/78.html"]
      tags: ["owasp", "llm02", "insecure-output", "command-injection", "subprocess", "os.system", "llm", "ai-security", "cohere", "ai-enhanced"]
      technology: ["python", "cohere", "llm"]
      confidence: "high"
      impact: "critical"
      likelihood: "high"
      description: "Cohere chat or generate output is extracted (from .text, .message.content, or .generations[0].text) and passed directly to command execution functions (subprocess, os.system). This allows arbitrary command execution if the LLM output contains malicious commands."
      remediation: "Never execute LLM output as commands. Use safe parsing, validation, and structured output formats. If command execution is needed, use allowlists, parameterized commands, or sandboxed execution environments."
      ai_analysis_recommended: true
      examples:
        vulnerable: "samples/cohere_command_injection.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://cwe.mitre.org/data/definitions/78.html"
    paths:
      include:
        - "*.py"
