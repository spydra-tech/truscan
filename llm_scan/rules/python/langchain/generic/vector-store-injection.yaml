rules:
  # ============================================================================
  # LLM01/LLM03: Vector Store Injection & Training Data Poisoning
  # ============================================================================
  # Detects vulnerabilities in vector store operations that can lead to
  # prompt injection or training data poisoning
  # 
  # Usage with AI:
  #   python -m llm_scan.runner . --enable-ai-filter \
  #     --ai-analyze-rules langchain-llm01-vector-store-user-input \
  #     --ai-analyze-rules langchain-llm03-vector-store-untrusted-data
  #
  # OWASP: LLM01, LLM03
  # CWE: CWE-79, CWE-94, CWE-502
  # ============================================================================

  - id: langchain-llm01-vector-store-user-input
    patterns:
      - pattern-either:
          # User input in similarity search
          - pattern: |
              $VECTORSTORE.similarity_search($USER_INPUT)
          - pattern: |
              $VECTORSTORE.similarity_search_with_score($USER_INPUT)
          - pattern: |
              $VECTORSTORE.similarity_search_by_vector($USER_INPUT)
          - pattern: |
              $RETRIEVER.get_relevant_documents($USER_INPUT)
          - pattern: |
              $RETRIEVER.invoke($USER_INPUT)
      - metavariable-regex:
          metavariable: $USER_INPUT
          regex: (request\.|input\(|sys\.argv|getenv|environ|args|kwargs|params|query|body|form|json|data|user_input|user_input_text|user_message|user_prompt|prompt_text|raw_input|raw_prompt|user_data|user_content|user_query|query|search_query)
    message: "LLM01: Potential Prompt Injection - User input in vector store similarity search"
    severity: WARNING
    languages: [python]
    metadata:
      category: security
      subcategory: injection
      owasp: "LLM01"
      owasp-title: "Prompt Injection (Vector Store)"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM01.html"
      cwe: ["CWE-79", "CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/79.html", "https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["owasp", "llm01", "prompt-injection", "vector-store", "langchain", "ai-enhanced"]
      technology: ["python", "langchain", "llm"]
      confidence: "medium"  # Medium confidence - depends on how results are used
      impact: "high"
      likelihood: "medium"  # AI will assess based on result usage
      description: "User input is used in vector store similarity search. If the retrieved documents are injected into prompts without sanitization, this can enable indirect prompt injection. AI analysis will determine if the search results are used unsafely in prompts."
      remediation: "Validate and sanitize user queries before vector search. Sanitize retrieved document content before including in prompts. Use delimiters to separate retrieved content from instructions. Implement content filtering for retrieved documents."
      ai_analysis_recommended: false
      examples:
        vulnerable: "samples/langchain_vector_store.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
    paths:
      include:
        - "**/*.py"

  - id: langchain-llm03-vector-store-untrusted-data
    patterns:
      - pattern-either:
          # Untrusted data in vector store creation
          - pattern: |
              $VECTORSTORE.from_texts($TEXTS, ...)
          - pattern: |
              $VECTORSTORE.from_documents($DOCUMENTS, ...)
      - pattern-inside: |
          $TEXTS = ...
          ...
          $VECTORSTORE.from_texts($TEXTS, ...)
      - metavariable-regex:
          metavariable: $TEXTS
          regex: (requests\.|urlopen|urllib|get\(|\.text|\.content|read\(|fetch|download|user|untrusted|external|third_party)
    message: "LLM03: Training Data Poisoning - Untrusted data in vector store creation"
    severity: WARNING
    languages: [python]
    metadata:
      category: security
      subcategory: training-data-poisoning
      owasp: "LLM03"
      owasp-title: "Training Data Poisoning"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM03.html"
      cwe: ["CWE-502"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/502.html"]
      tags: ["owasp", "llm03", "training-data-poisoning", "vector-store", "langchain", "ai-enhanced"]
      technology: ["python", "langchain", "llm"]
      confidence: "medium"  # Medium confidence - depends on data source
      impact: "high"
      likelihood: "medium"  # AI will assess data source trustworthiness
      description: "Vector store is created from potentially untrusted data sources (web requests, external files, user input). This can lead to training data poisoning where malicious content is embedded in the vector store and later retrieved into prompts. AI analysis will determine if the data source is actually untrusted."
      remediation: "Validate and sanitize all data before adding to vector stores. Use trusted data sources only. Implement content filtering and validation for external data. Verify data integrity and authenticity before embedding."
      ai_analysis_recommended: false
      examples:
        vulnerable: "samples/langchain_vector_store.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
    paths:
      include:
        - "**/*.py"

  - id: langchain-llm01-vector-store-to-prompt
    mode: taint
    pattern-sources:
      - pattern: |
          $VECTORSTORE.similarity_search(...)
      - pattern: |
          $VECTORSTORE.similarity_search_with_score(...)
      - pattern: |
          $RETRIEVER.get_relevant_documents(...)
      - pattern: |
          $RETRIEVER.invoke(...)
    pattern-sinks:
      - pattern: |
          ChatPromptTemplate.from_template(...)
      - pattern: |
          PromptTemplate.from_template(...)
      - pattern: |
          $CHAIN.run(...)
      - pattern: |
          $CHAIN.invoke(...)
    message: "LLM01: Indirect Prompt Injection - Vector store results flow into prompts without sanitization"
    severity: WARNING
    languages: [python]
    metadata:
      category: security
      subcategory: injection
      owasp: "LLM01"
      owasp-title: "Prompt Injection (Indirect)"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM01.html"
      cwe: ["CWE-79", "CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/79.html", "https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["owasp", "llm01", "prompt-injection", "indirect-injection", "vector-store", "langchain", "taint", "ai-enhanced"]
      technology: ["python", "langchain", "llm"]
      confidence: "medium"  # Medium confidence - depends on sanitization
      impact: "high"
      likelihood: "medium"  # AI will assess if sanitization is present
      description: "Vector store search results flow into LangChain prompts or chains. If the vector store contains poisoned or malicious content, this enables indirect prompt injection. AI analysis will determine if the retrieved content is properly sanitized before use in prompts."
      remediation: "Sanitize all retrieved document content before including in prompts. Use delimiters to separate retrieved content from instructions. Implement content filtering and validation for vector store results. Treat all retrieved content as potentially malicious."
      ai_analysis_recommended: false
      examples:
        vulnerable: "samples/langchain_vector_store.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
    paths:
      include:
        - "**/*.py"
