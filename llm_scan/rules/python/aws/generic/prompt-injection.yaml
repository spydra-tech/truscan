rules:
  # ============================================================================
  # LLM01: Prompt Injection - AWS Bedrock Generic (AI-Enhanced Detection)
  # ============================================================================
  # This rule set is designed to work with AI analysis to reduce false positives.
  # Semgrep catches potential patterns broadly, AI determines exploitability.
  # 
  # Usage with AI:
  #   python -m llm_scan.runner . --enable-ai-filter \
  #     --ai-analyze-rules aws-llm01-prompt-injection-converse \
  #     --ai-analyze-rules aws-llm01-prompt-injection-invoke-model \
  #     --ai-analyze-rules aws-llm01-prompt-injection-concatenation
  #
  # OWASP: LLM01
  # CWE: CWE-79, CWE-94
  # ============================================================================

  - id: aws-llm01-prompt-injection-converse
    patterns:
      - pattern-either:
          # Direct user input in Converse API messages
          - pattern: |
              $BEDROCK.converse(messages=[..., {"role": "user", "content": $USER_INPUT}, ...])
          - pattern: |
              $BEDROCK.converse(messages=[..., {"content": $USER_INPUT}, ...])
          - pattern: |
              boto3.client('bedrock-runtime').converse(messages=[..., {"role": "user", "content": $USER_INPUT}, ...])
          - pattern: |
              $CLIENT.converse(messages=[..., {"role": "user", "content": $USER_INPUT}, ...])
          - pattern: |
              $CLIENT.converse_stream(messages=[..., {"role": "user", "content": $USER_INPUT}, ...])
      # Only exclude well-known standard library/framework escaping functions
      # AI will analyze if custom sanitization is sufficient
      - pattern-not-inside: |
          $SANITIZED = html.escape($USER_INPUT)
          ...
      - pattern-not-inside: |
          $SANITIZED = markupsafe.escape($USER_INPUT)
          ...
      - pattern-not-inside: |
          $SANITIZED = bleach.clean($USER_INPUT)
          ...
      - metavariable-regex:
          metavariable: $USER_INPUT
          regex: (request\.|input\(|sys\.argv|getenv|environ|args|kwargs|params|query|body|form|json|data|user_input|user_input_text|user_message|user_prompt|prompt_text|raw_input|raw_prompt|user_data|user_content)
    message: "LLM01: Potential Prompt Injection - User input directly inserted into AWS Bedrock Converse prompt"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: injection
      owasp: "LLM01"
      owasp-title: "Prompt Injection"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM01.html"
      cwe: ["CWE-79", "CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/79.html", "https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["owasp", "llm01", "prompt-injection", "injection", "input-validation", "llm", "ai-security", "aws", "bedrock", "ai-enhanced"]
      technology: ["python", "aws", "bedrock", "boto3", "llm"]
      confidence: "medium"  # Medium confidence - AI will analyze to determine if exploitable
      impact: "critical"
      likelihood: "medium"  # AI will assess actual likelihood based on context
      description: "User input is directly inserted into AWS Bedrock Converse API prompts. This pattern may be vulnerable to prompt injection if input is not properly sanitized or validated. AI analysis will determine if the vulnerability is actually exploitable based on code context, sanitization, and validation."
      remediation: "Sanitize and validate all user input before including in prompts. Use prompt templates with parameterized inputs. Implement input validation, output encoding, and prompt engineering best practices. Consider using delimiters to separate user data from instructions. Enable AWS Bedrock Guardrails for additional protection."
      ai_analysis_recommended: false
      examples:
        vulnerable: "samples/aws_bedrock_prompt_injection.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://learnprompting.org/docs/category/-prompt-injection"
        - "https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-injection.html"
        - "https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html"
    paths:
      include:
        - "**/*.py"

  - id: aws-llm01-prompt-injection-invoke-model
    patterns:
      - pattern-either:
          # User input in InvokeModel body
          - pattern: |
              $BEDROCK.invoke_model(modelId=..., body=json.dumps({"prompt": $USER_INPUT}))
          - pattern: |
              $BEDROCK.invoke_model(modelId=..., body=json.dumps({..., "prompt": $USER_INPUT, ...}))
          - pattern: |
              $CLIENT.invoke_model(modelId=..., body=$BODY)
              ...
              $BODY = json.dumps({"prompt": $USER_INPUT})
          - pattern: |
              $CLIENT.invoke_model(modelId=..., body=$BODY)
              ...
              $BODY = json.dumps({..., "prompt": $USER_INPUT, ...})
          - pattern: |
              $BEDROCK.invoke_model_with_response_stream(modelId=..., body=json.dumps({"prompt": $USER_INPUT}))
      - metavariable-regex:
          metavariable: $USER_INPUT
          regex: (request\.|input\(|sys\.argv|getenv|environ|args|kwargs|params|query|body|form|json|data|user_input|user_input_text|user_message|user_prompt|prompt_text|raw_input|raw_prompt|user_data|user_content)
    message: "LLM01: Potential Prompt Injection - User input directly inserted into AWS Bedrock InvokeModel prompt"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: injection
      owasp: "LLM01"
      owasp-title: "Prompt Injection"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM01.html"
      cwe: ["CWE-79", "CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/79.html", "https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["owasp", "llm01", "prompt-injection", "injection", "input-validation", "llm", "ai-security", "aws", "bedrock", "ai-enhanced"]
      technology: ["python", "aws", "bedrock", "boto3", "llm"]
      confidence: "medium"  # Medium confidence - AI will analyze to determine if exploitable
      impact: "critical"
      likelihood: "medium"  # AI will assess actual likelihood based on context
      description: "User input is directly inserted into AWS Bedrock InvokeModel API body. This pattern may be vulnerable to prompt injection if input is not properly sanitized or validated. AI analysis will determine if the vulnerability is actually exploitable based on code context, sanitization, and validation."
      remediation: "Sanitize and validate all user input before including in prompts. Use prompt templates with parameterized inputs. Implement input validation, output encoding, and prompt engineering best practices. Consider using delimiters to separate user data from instructions. Enable AWS Bedrock Guardrails for additional protection."
      ai_analysis_recommended: false
      examples:
        vulnerable: "samples/aws_bedrock_prompt_injection.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://learnprompting.org/docs/category/-prompt-injection"
        - "https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-injection.html"
    paths:
      include:
        - "**/*.py"

  - id: aws-llm01-prompt-injection-concatenation
    patterns:
      - pattern-either:
          # String concatenation in Converse messages
          - pattern: |
              $BEDROCK.converse(messages=[..., {"role": "user", "content": $PROMPT + $USER_INPUT}, ...])
          - pattern: |
              $BEDROCK.converse(messages=[..., {"content": $PROMPT + $USER_INPUT}, ...])
          - pattern: |
              $CLIENT.converse(messages=[..., {"role": "user", "content": f"...{$USER_INPUT}..."}, ...])
          - pattern: |
              $CLIENT.converse(messages=[..., {"content": f"...{$USER_INPUT}..."}, ...])
          # String concatenation in InvokeModel body
          - pattern: |
              $BEDROCK.invoke_model(modelId=..., body=json.dumps({"prompt": $PROMPT + $USER_INPUT}))
          - pattern: |
              $BEDROCK.invoke_model(modelId=..., body=json.dumps({"prompt": f"...{$USER_INPUT}..."}))
          # Variable assignment before API call
          - pattern: |
              $PROMPT = ... + $USER_INPUT + ...
              ...
              $BEDROCK.converse(messages=[..., {"content": $PROMPT}, ...])
          - pattern: |
              $BODY = json.dumps({"prompt": f"...{$USER_INPUT}..."})
              ...
              $BEDROCK.invoke_model(modelId=..., body=$BODY)
      - metavariable-regex:
          metavariable: $USER_INPUT
          regex: (request\.|input\(|sys\.argv|getenv|environ|args|kwargs|params|query|body|form|json|data|user_input|user_input_text|user_message|user_prompt|prompt_text|raw_input|raw_prompt|user_data|user_content)
    message: "LLM01: Potential Prompt Injection - User input concatenated into AWS Bedrock prompt"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: injection
      owasp: "LLM01"
      owasp-title: "Prompt Injection"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM01.html"
      cwe: ["CWE-79", "CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/79.html", "https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["owasp", "llm01", "prompt-injection", "injection", "input-validation", "llm", "ai-security", "aws", "bedrock", "ai-enhanced"]
      technology: ["python", "aws", "bedrock", "boto3", "llm"]
      confidence: "medium"  # Medium confidence - AI will analyze to determine if exploitable
      impact: "critical"
      likelihood: "medium"  # AI will assess actual likelihood based on context
      description: "User input is concatenated into AWS Bedrock prompts using string concatenation or f-strings. This pattern may be vulnerable to prompt injection if input is not properly sanitized or validated. AI analysis will determine if the vulnerability is actually exploitable based on code context, sanitization, and validation."
      remediation: "Sanitize and validate all user input before including in prompts. Use prompt templates with parameterized inputs. Implement input validation, output encoding, and prompt engineering best practices. Consider using delimiters to separate user data from instructions. Enable AWS Bedrock Guardrails for additional protection."
      ai_analysis_recommended: false
      examples:
        vulnerable: "samples/aws_bedrock_prompt_injection.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://learnprompting.org/docs/category/-prompt-injection"
        - "https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-injection.html"
    paths:
      include:
        - "**/*.py"

  - id: aws-llm01-prompt-injection-agent
    patterns:
      - pattern-either:
          # User input in Bedrock Agent invocation
          - pattern: |
              $AGENT.invoke_agent(inputText=$USER_INPUT, ...)
          - pattern: |
              $AGENT.invoke_agent(..., inputText=$USER_INPUT, ...)
          - pattern: |
              boto3.client('bedrock-agent-runtime').invoke_agent(inputText=$USER_INPUT, ...)
          - pattern: |
              $CLIENT.invoke_agent(inputText=$USER_INPUT, ...)
      - metavariable-regex:
          metavariable: $USER_INPUT
          regex: (request\.|input\(|sys\.argv|getenv|environ|args|kwargs|params|query|body|form|json|data|user_input|user_input_text|user_message|user_prompt|prompt_text|raw_input|raw_prompt|user_data|user_content)
    message: "LLM01: Potential Prompt Injection - User input in AWS Bedrock Agent invocation"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: injection
      owasp: "LLM01"
      owasp-title: "Prompt Injection (Bedrock Agent)"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM01.html"
      cwe: ["CWE-79", "CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/79.html", "https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["owasp", "llm01", "prompt-injection", "injection", "bedrock-agent", "llm", "ai-security", "aws", "bedrock", "ai-enhanced"]
      technology: ["python", "aws", "bedrock", "boto3", "llm"]
      confidence: "medium"  # Medium confidence - AI will analyze to determine if exploitable
      impact: "critical"
      likelihood: "medium"  # AI will assess actual likelihood based on context
      description: "User input is passed to AWS Bedrock Agent invocation. Bedrock Agents can execute tools and access resources, making prompt injection particularly dangerous. AI analysis will determine if the vulnerability is actually exploitable based on code context, sanitization, and validation."
      remediation: "Sanitize and validate all user input before passing to Bedrock Agents. Implement input validation and output encoding. Use AWS Bedrock Guardrails for additional protection. Limit agent tool access and permissions."
      ai_analysis_recommended: false
      examples:
        vulnerable: "samples/aws_bedrock_agent_injection.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-injection.html"
        - "https://aws.amazon.com/blogs/machine-learning/securing-amazon-bedrock-agents-a-guide-to-safeguarding-against-indirect-prompt-injections/"
    paths:
      include:
        - "**/*.py"
