rules:
  # ============================================================================
  # LLM02: Code Injection - AWS Bedrock Generic
  # ============================================================================
  # Detects LLM output used in code execution functions
  # OWASP: LLM02
  # CWE: CWE-94
  # ============================================================================

  - id: aws-code-injection-eval
    patterns:
      - pattern-either:
          - pattern: |
              $RESPONSE = $BEDROCK.converse(...)
          - pattern: |
              $RESPONSE = $CLIENT.converse(...)
          - pattern: |
              $RESPONSE = $BEDROCK.invoke_model(...)
          - pattern: |
              $RESPONSE = $CLIENT.invoke_model(...)
          - pattern: |
              $RESPONSE = boto3.client('bedrock-runtime').converse(...)
      - pattern-either:
          - pattern: eval($VAR)
          - pattern: exec($VAR)
          - pattern: compile($VAR, ...)
      - metavariable-regex:
          metavariable: $VAR
          regex: (code|content|text|output|result|response|command|cmd|str|message|data|resp|answer|\.text|\.content|completion|message\[.*\]\.content|output\.message\.content)
    message: "LLM02: Insecure Output Handling - AWS Bedrock output passed to eval/exec/compile (code injection risk)"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: code-injection
      owasp: "LLM02"
      owasp-title: "Insecure Output Handling"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM02.html"
      cwe: ["CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["owasp", "llm02", "insecure-output", "code-injection", "eval", "exec", "compile", "llm", "ai-security", "aws", "bedrock"]
      technology: ["python", "aws", "bedrock", "boto3", "llm"]
      confidence: "high"
      impact: "critical"
      likelihood: "high"
      description: "AWS Bedrock output is passed to eval(), exec(), or compile() functions without validation, allowing arbitrary code execution if the LLM output contains malicious code."
      remediation: "Never execute LLM output as code. Use safe parsing and validation instead. Use AST parsing, whitelisted operations, or structured output formats (JSON, YAML) instead of code execution."
      ai_analysis_recommended: true
      examples:
        vulnerable: "samples/aws_bedrock_code_injection.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://cwe.mitre.org/data/definitions/94.html"
    paths:
      include:
        - "*.py"

  - id: aws-code-injection-direct
    patterns:
      - pattern-either:
          # Converse response to eval/exec/compile
          - pattern: |
              $RESPONSE = $BEDROCK.converse(...)
              ...
              $BODY = json.loads($RESPONSE['body'].read())
              ...
              $VAR = $BODY['output']['message']['content'][0]['text']
              ...
              eval($VAR)
          - pattern: |
              $RESPONSE = $CLIENT.converse(...)
              ...
              $BODY = json.loads($RESPONSE['body'].read())
              ...
              $VAR = $BODY['output']['message']['content'][0]['text']
              ...
              exec($VAR)
          # InvokeModel response to eval/exec/compile
          - pattern: |
              $RESPONSE = $BEDROCK.invoke_model(...)
              ...
              $BODY = json.loads($RESPONSE['body'].read())
              ...
              $VAR = $BODY['completion']
              ...
              eval($VAR)
          - pattern: |
              $RESPONSE = $CLIENT.invoke_model(...)
              ...
              $BODY = json.loads($RESPONSE['body'].read())
              ...
              $VAR = $BODY['completion']
              ...
              exec($VAR)
          - pattern: |
              $RESPONSE = $BEDROCK.converse(...)
              ...
              $VAR = json.loads($RESPONSE['body'].read())['output']['message']['content'][0]['text']
              ...
              compile($VAR, ...)
    message: "LLM02: Insecure Output Handling - AWS Bedrock output extracted and passed to eval/exec/compile"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: code-injection
      owasp: "LLM02"
      owasp-title: "Insecure Output Handling"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM02.html"
      cwe: ["CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["owasp", "llm02", "insecure-output", "code-injection", "eval", "exec", "compile", "llm", "ai-security", "aws", "bedrock", "ai-enhanced"]
      technology: ["python", "aws", "bedrock", "boto3", "llm"]
      confidence: "high"
      impact: "critical"
      likelihood: "high"
      description: "AWS Bedrock output is extracted (from JSON response body) and passed directly to eval(), exec(), or compile() functions. This allows arbitrary code execution if the LLM output contains malicious code."
      remediation: "Never execute LLM output as code. Use safe parsing and validation instead. Use AST parsing, whitelisted operations, or structured output formats (JSON, YAML) instead of code execution."
      ai_analysis_recommended: true
      examples:
        vulnerable: "samples/aws_bedrock_code_injection.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://cwe.mitre.org/data/definitions/94.html"
    paths:
      include:
        - "*.py"
