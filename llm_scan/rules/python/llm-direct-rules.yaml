rules:
  # Direct pattern: eval after any assignment that might come from LLM
  - id: llm-eval-direct
    pattern-either:
      - pattern: |
          $RESPONSE = openai.ChatCompletion.create(...)
          ...
          $VAR = $RESPONSE.choices[0].message.content
          ...
          eval($VAR)
      - pattern: |
          $RESPONSE = openai.Completion.create(...)
          ...
          $VAR = $RESPONSE.choices[0].text
          ...
          eval($VAR)
      - pattern: |
          $RESPONSE = $CLIENT.chat.completions.create(...)
          ...
          $VAR = $RESPONSE.choices[0].message.content
          ...
          eval($VAR)
    message: "LLM output extracted and passed to eval() - CODE INJECTION"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: code-injection
      cwe: ["CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["code-injection", "eval", "llm", "ai-security", "direct", "injection"]
      technology: ["python", "openai", "anthropic", "llm"]
      confidence: "high"
      impact: "critical"
      likelihood: "high"
      description: "LLM output is directly extracted from response and passed to eval() without validation, allowing code injection."
      remediation: "Never execute LLM output as code. Validate and sanitize all LLM outputs. Use structured output formats."
      examples:
        vulnerable: "samples/vulnerable_app.py"
      references:
        - "https://owasp.org/www-community/vulnerabilities/Code_Injection"
        - "https://cwe.mitre.org/data/definitions/94.html"
    paths:
      include:
        - "*.py"

  - id: llm-exec-direct
    pattern-either:
      - pattern: |
          $RESPONSE = openai.ChatCompletion.create(...)
          ...
          $VAR = $RESPONSE.choices[0].message.content
          ...
          exec($VAR)
      - pattern: |
          $RESPONSE = openai.Completion.create(...)
          ...
          $VAR = $RESPONSE.choices[0].text
          ...
          exec($VAR)
      - pattern: |
          $RESPONSE = $CLIENT.chat.completions.create(...)
          ...
          $VAR = $RESPONSE.choices[0].message.content
          ...
          exec($VAR)
    message: "LLM output extracted and passed to exec() - CODE INJECTION"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: code-injection
      cwe: ["CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["code-injection", "exec", "llm", "ai-security", "direct", "injection"]
      technology: ["python", "openai", "anthropic", "llm"]
      confidence: "high"
      impact: "critical"
      likelihood: "high"
      description: "LLM output is directly extracted from response and passed to exec() without validation, allowing code injection."
      remediation: "Never execute LLM output as code. Validate and sanitize all LLM outputs. Use structured output formats."
      examples:
        vulnerable: "samples/vulnerable_app.py"
      references:
        - "https://owasp.org/www-community/vulnerabilities/Code_Injection"
        - "https://cwe.mitre.org/data/definitions/94.html"
    paths:
      include:
        - "*.py"

  - id: llm-compile-direct
    pattern-either:
      - pattern: |
          $RESPONSE = openai.ChatCompletion.create(...)
          ...
          $VAR = $RESPONSE.choices[0].message.content
          ...
          compile($VAR, ...)
      - pattern: |
          $RESPONSE = openai.Completion.create(...)
          ...
          $VAR = $RESPONSE.choices[0].text
          ...
          compile($VAR, ...)
      - pattern: |
          $RESPONSE = $CLIENT.chat.completions.create(...)
          ...
          $VAR = $RESPONSE.choices[0].message.content
          ...
          compile($VAR, ...)
    message: "LLM output extracted and passed to compile() - CODE INJECTION"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: code-injection
      cwe: ["CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["code-injection", "compile", "llm", "ai-security", "direct", "injection"]
      technology: ["python", "openai", "anthropic", "llm"]
      confidence: "high"
      impact: "critical"
      likelihood: "high"
      description: "LLM output is directly extracted from response and passed to compile() without validation, allowing code injection."
      remediation: "Never compile LLM output as code. Validate and sanitize all LLM outputs. Use structured output formats."
      examples:
        vulnerable: "samples/vulnerable_app.py"
      references:
        - "https://owasp.org/www-community/vulnerabilities/Code_Injection"
        - "https://cwe.mitre.org/data/definitions/94.html"
    paths:
      include:
        - "*.py"

  - id: llm-subprocess-direct
    pattern-either:
      - pattern: |
          $RESPONSE = openai.ChatCompletion.create(...)
          ...
          $VAR = $RESPONSE.choices[0].message.content
          ...
          subprocess.run($VAR, ...)
      - pattern: |
          $RESPONSE = openai.Completion.create(...)
          ...
          $VAR = $RESPONSE.choices[0].text
          ...
          subprocess.run($VAR, ...)
      - pattern: |
          $RESPONSE = $CLIENT.chat.completions.create(...)
          ...
          $VAR = $RESPONSE.choices[0].message.content
          ...
          subprocess.run($VAR, ...)
    message: "LLM output extracted and passed to subprocess.run() - COMMAND INJECTION"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: command-injection
      cwe: ["CWE-78"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/78.html"]
      tags: ["command-injection", "subprocess", "llm", "ai-security", "direct", "injection"]
      technology: ["python", "openai", "anthropic", "llm"]
      confidence: "high"
      impact: "critical"
      likelihood: "high"
      description: "LLM output is directly extracted from response and passed to subprocess functions without validation, allowing command injection."
      remediation: "Never execute LLM output as shell commands. Use subprocess with shell=False and validate all inputs."
      examples:
        vulnerable: "samples/vulnerable_app.py"
      references:
        - "https://owasp.org/www-community/attacks/Command_Injection"
        - "https://cwe.mitre.org/data/definitions/78.html"
    paths:
      include:
        - "*.py"

  - id: llm-os-system-direct
    pattern-either:
      - pattern: |
          $RESPONSE = openai.ChatCompletion.create(...)
          ...
          $VAR = $RESPONSE.choices[0].message.content
          ...
          os.system($VAR)
      - pattern: |
          $RESPONSE = openai.Completion.create(...)
          ...
          $VAR = $RESPONSE.choices[0].text
          ...
          os.system($VAR)
      - pattern: |
          $RESPONSE = $CLIENT.chat.completions.create(...)
          ...
          $VAR = $RESPONSE.choices[0].message.content
          ...
          os.system($VAR)
    message: "LLM output extracted and passed to os.system() - COMMAND INJECTION"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: command-injection
      cwe: ["CWE-78"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/78.html"]
      tags: ["command-injection", "subprocess", "llm", "ai-security", "direct", "injection"]
      technology: ["python", "openai", "anthropic", "llm"]
      confidence: "high"
      impact: "critical"
      likelihood: "high"
      description: "LLM output is directly extracted from response and passed to subprocess functions without validation, allowing command injection."
      remediation: "Never execute LLM output as shell commands. Use subprocess with shell=False and validate all inputs."
      examples:
        vulnerable: "samples/vulnerable_app.py"
      references:
        - "https://owasp.org/www-community/attacks/Command_Injection"
        - "https://cwe.mitre.org/data/definitions/78.html"
    paths:
      include:
        - "*.py"

  - id: llm-anthropic-eval-direct
    pattern: |
      $RESPONSE = $CLIENT.messages.create(...)
      ...
      $VAR = $RESPONSE.content[0].text
      ...
      eval($VAR)
    message: "Anthropic LLM output extracted and passed to eval() - CODE INJECTION"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: code-injection
      cwe: ["CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["code-injection", "eval", "llm", "ai-security", "direct", "injection"]
      technology: ["python", "openai", "anthropic", "llm"]
      confidence: "high"
      impact: "critical"
      likelihood: "high"
      description: "LLM output is directly extracted from response and passed to eval() without validation, allowing code injection."
      remediation: "Never execute LLM output as code. Validate and sanitize all LLM outputs. Use structured output formats."
      examples:
        vulnerable: "samples/vulnerable_app.py"
      references:
        - "https://owasp.org/www-community/vulnerabilities/Code_Injection"
        - "https://cwe.mitre.org/data/definitions/94.html"
    paths:
      include:
        - "*.py"

  - id: llm-anthropic-subprocess-direct
    pattern: |
      $RESPONSE = $CLIENT.messages.create(...)
      ...
      $VAR = $RESPONSE.content[0].text
      ...
      subprocess.run($VAR, ...)
    message: "Anthropic LLM output extracted and passed to subprocess.run() - COMMAND INJECTION"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: command-injection
      cwe: ["CWE-78"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/78.html"]
      tags: ["command-injection", "subprocess", "llm", "ai-security", "direct", "injection"]
      technology: ["python", "openai", "anthropic", "llm"]
      confidence: "high"
      impact: "critical"
      likelihood: "high"
      description: "LLM output is directly extracted from response and passed to subprocess functions without validation, allowing command injection."
      remediation: "Never execute LLM output as shell commands. Use subprocess with shell=False and validate all inputs."
      examples:
        vulnerable: "samples/vulnerable_app.py"
      references:
        - "https://owasp.org/www-community/attacks/Command_Injection"
        - "https://cwe.mitre.org/data/definitions/78.html"
    paths:
      include:
        - "*.py"
