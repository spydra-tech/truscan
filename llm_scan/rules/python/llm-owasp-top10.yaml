rules:
  # ============================================================================
  # OWASP Top 10 for LLM Applications
  # ============================================================================
  # Reference: https://owasp.org/www-project-top-10-for-large-language-model-applications/
  # ============================================================================

  # ----------------------------------------------------------------------------
  # LLM01: Prompt Injection
  # ----------------------------------------------------------------------------
  # Description: Directly inserting untrusted input into prompts without
  #              sanitization, allowing attackers to manipulate LLM behavior
  # CWE: CWE-79, CWE-94
  # ----------------------------------------------------------------------------
  - id: llm01-prompt-injection-direct
    patterns:
      - pattern-either:
          - pattern: |
              openai.ChatCompletion.create(messages=[..., {"role": "user", "content": $USER_INPUT}, ...])
          - pattern: |
              openai.Completion.create(prompt=$USER_INPUT, ...)
          - pattern: |
              $CLIENT.chat.completions.create(messages=[..., {"role": "user", "content": $USER_INPUT}, ...])
          - pattern: |
              $CLIENT.completions.create(prompt=$USER_INPUT, ...)
          - pattern: |
              $CLIENT.messages.create(messages=[..., {"content": $USER_INPUT}, ...])
      - pattern-not-inside: |
          $SANITIZED = sanitize($USER_INPUT)
          ...
      - pattern-not-inside: |
          $SANITIZED = escape($USER_INPUT)
          ...
      - metavariable-regex:
          metavariable: $USER_INPUT
          regex: (request\.|input\(|sys\.argv|getenv|environ|args|kwargs|params|query|body|form|json|data|user_input|user_input_text|user_message|user_prompt|prompt_text|raw_input|raw_prompt)
    message: "LLM01: Prompt Injection - User input directly inserted into LLM prompt without sanitization"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: injection
      owasp: "LLM01"
      owasp-title: "Prompt Injection"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM01.html"
      cwe: ["CWE-79", "CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/79.html", "https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["owasp", "llm01", "prompt-injection", "injection", "input-validation", "llm", "ai-security"]
      technology: ["python", "openai", "anthropic", "llm"]
      confidence: "high"
      impact: "critical"
      likelihood: "high"
      description: "User input is directly inserted into LLM prompts without sanitization, allowing attackers to manipulate LLM behavior, bypass system prompts, or inject malicious instructions."
      remediation: "Sanitize and validate all user input before including in prompts. Use prompt templates with parameterized inputs. Implement input validation, output encoding, and prompt engineering best practices."
      examples:
        vulnerable: "samples/llm01_prompt_injection.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://learnprompting.org/docs/category/-prompt-injection"
    paths:
      include:
        - "*.py"

  - id: llm01-prompt-injection-concatenation
    patterns:
      - pattern-either:
          - pattern: |
              openai.ChatCompletion.create(messages=[..., {"role": "user", "content": $PROMPT + $USER_INPUT}, ...])
          - pattern: |
              openai.Completion.create(prompt=$PROMPT + $USER_INPUT, ...)
          - pattern: |
              $CLIENT.chat.completions.create(messages=[..., {"role": "user", "content": $PROMPT + $USER_INPUT}, ...])
          - pattern: $PROMPT = ... + $USER_INPUT + ...
          - pattern: f"...{$USER_INPUT}..."
      - pattern-inside: |
          $RESPONSE = openai.ChatCompletion.create(...)
          ...
      - metavariable-regex:
          metavariable: $USER_INPUT
          regex: (request\.|input\(|sys\.argv|getenv|environ|args|kwargs|params|query|body|form|json|data|user_input|user_input_text|user_message|user_prompt|prompt_text|raw_input|raw_prompt)
    message: "LLM01: Prompt Injection - User input concatenated into prompt without sanitization"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: injection
      owasp: "LLM01"
      owasp-title: "Prompt Injection"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM01.html"
      cwe: ["CWE-79", "CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/79.html", "https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["owasp", "llm01", "prompt-injection", "injection", "input-validation", "llm", "ai-security", "string-concatenation"]
      technology: ["python", "openai", "anthropic", "llm"]
      confidence: "high"
      impact: "critical"
      likelihood: "high"
      description: "User input is concatenated into LLM prompts using string operations (+, f-strings) without sanitization, making prompt injection attacks easier."
      remediation: "Use parameterized prompt templates instead of string concatenation. Validate and sanitize all user inputs. Avoid f-strings or + operators with user input in prompts."
      examples:
        vulnerable: "samples/llm01_prompt_injection.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://learnprompting.org/docs/category/-prompt-injection"
    paths:
      include:
        - "*.py"

  # ----------------------------------------------------------------------------
  # LLM02: Insecure Output Handling
  # ----------------------------------------------------------------------------
  # Description: LLM output used unsafely without validation, leading to
  #              code injection, command injection, XSS, etc.
  # CWE: CWE-94, CWE-78, CWE-79
  # Note: This is already covered by llm-code-injection.yaml and
  #       llm-command-injection.yaml, but we add OWASP-specific rules here
  # ----------------------------------------------------------------------------
  - id: llm02-insecure-output-eval
    patterns:
      - pattern-either:
          - pattern: |
              $RESPONSE = openai.ChatCompletion.create(...)
          - pattern: |
              $RESPONSE = openai.Completion.create(...)
          - pattern: |
              $RESPONSE = $CLIENT.chat.completions.create(...)
          - pattern: |
              $RESPONSE = $CLIENT.messages.create(...)
      - pattern-inside: |
          $CONTENT = $RESPONSE.choices[0].message.content
          ...
          eval($CONTENT)
    message: "LLM02: Insecure Output Handling - LLM output passed to eval() without validation"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: code-injection
      owasp: "LLM02"
      owasp-title: "Insecure Output Handling"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM02.html"
      cwe: ["CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["owasp", "llm02", "insecure-output", "code-injection", "eval", "exec", "llm", "ai-security"]
      technology: ["python", "openai", "anthropic", "llm"]
      confidence: "high"
      impact: "critical"
      likelihood: "high"
      description: "LLM output is passed to eval() without validation, allowing arbitrary code execution if the LLM output contains malicious code."
      remediation: "Never execute LLM output as code. Validate, sanitize, and use safe parsing methods. Use AST parsing or whitelisted operations instead of eval/exec."
      examples:
        vulnerable: "samples/llm02_insecure_output.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://owasp.org/www-community/vulnerabilities/Code_Injection"
    paths:
      include:
        - "*.py"

  - id: llm02-insecure-output-subprocess
    patterns:
      - pattern-either:
          - pattern: |
              $RESPONSE = openai.ChatCompletion.create(...)
          - pattern: |
              $RESPONSE = openai.Completion.create(...)
          - pattern: |
              $RESPONSE = $CLIENT.chat.completions.create(...)
          - pattern: |
              $RESPONSE = $CLIENT.messages.create(...)
      - pattern-inside: |
          $CONTENT = $RESPONSE.choices[0].message.content
          ...
          subprocess.run($CONTENT, ...)
    message: "LLM02: Insecure Output Handling - LLM output passed to subprocess without validation"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: command-injection
      owasp: "LLM02"
      owasp-title: "Insecure Output Handling"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM02.html"
      cwe: ["CWE-78"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/78.html"]
      tags: ["owasp", "llm02", "insecure-output", "command-injection", "subprocess", "llm", "ai-security"]
      technology: ["python", "openai", "anthropic", "llm"]
      confidence: "high"
      impact: "critical"
      likelihood: "high"
      description: "LLM output is passed to subprocess.run() without validation, allowing command injection if the LLM output contains shell commands."
      remediation: "Never execute LLM output as shell commands. Use subprocess with shell=False and validate all inputs. Use command whitelists and parameterized commands."
      examples:
        vulnerable: "samples/llm02_insecure_output.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://owasp.org/www-community/attacks/Command_Injection"
    paths:
      include:
        - "*.py"

  - id: llm02-insecure-output-xss
    patterns:
      - pattern-either:
          - pattern: |
              $RESPONSE = openai.ChatCompletion.create(...)
          - pattern: |
              $RESPONSE = openai.Completion.create(...)
          - pattern: |
              $RESPONSE = $CLIENT.chat.completions.create(...)
      - pattern-inside: |
          $CONTENT = $RESPONSE.choices[0].message.content
          ...
          return render_template(..., content=$CONTENT)
          ...
      - pattern-not-inside: |
          $ESCAPED = escape($CONTENT)
          ...
    message: "LLM02: Insecure Output Handling - LLM output rendered in HTML without escaping (XSS risk)"
    severity: WARNING
    languages: [python]
    metadata:
      category: security
      subcategory: xss
      owasp: "LLM02"
      owasp-title: "Insecure Output Handling"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM02.html"
      cwe: ["CWE-79"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/79.html"]
      tags: ["owasp", "llm02", "insecure-output", "xss", "cross-site-scripting", "llm", "ai-security"]
      technology: ["python", "openai", "anthropic", "llm", "flask", "django"]
      confidence: "high"
      impact: "high"
      likelihood: "medium"
      description: "LLM output is rendered in HTML templates without escaping, allowing XSS attacks if the LLM output contains malicious JavaScript."
      remediation: "Always escape LLM output when rendering in HTML. Use framework's built-in escaping mechanisms (Jinja2 autoescape, Django's escape filter)."
      examples:
        vulnerable: "samples/llm02_insecure_output.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://owasp.org/www-community/attacks/xss/"
    paths:
      include:
        - "*.py"

  # ----------------------------------------------------------------------------
  # LLM03: Training Data Poisoning
  # ----------------------------------------------------------------------------
  # Description: Training data sourced from untrusted or unvalidated sources,
  #              allowing attackers to inject malicious patterns into the model
  # CWE: CWE-502, CWE-506
  # ----------------------------------------------------------------------------
  - id: llm03-training-data-untrusted-source
    patterns:
      - pattern-either:
          - pattern: |
              $DATA = requests.get($URL, ...)
              ...
              train_model(data=$DATA)
          - pattern: |
              $DATA = urllib.request.urlopen($URL)
              ...
              train_model(data=$DATA)
          - pattern: |
              $DATA = open($FILE, ...)
              ...
              train_model(data=$DATA)
      - pattern-not-inside: |
          $VALIDATED = validate_training_data($DATA)
          ...
      - metavariable-regex:
          metavariable: $URL
          regex: (http://|https://)
      - metavariable-regex:
          metavariable: $FILE
          regex: (user|tmp|download|external|untrusted)
    message: "LLM03: Training Data Poisoning - Training data loaded from untrusted source without validation"
    severity: WARNING
    languages: [python]
    metadata:
      category: security
      subcategory: data-poisoning
      owasp: "LLM03"
      owasp-title: "Training Data Poisoning"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM03.html"
      cwe: ["CWE-502", "CWE-506"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/502.html", "https://cwe.mitre.org/data/definitions/506.html"]
      tags: ["owasp", "llm03", "training-data", "data-poisoning", "supply-chain", "llm", "ai-security", "ml-security"]
      technology: ["python", "ml", "llm", "training"]
      confidence: "medium"
      impact: "high"
      likelihood: "medium"
      description: "Training data is loaded from untrusted sources (URLs, user files, external directories) without validation, checksum verification, or signature checking, allowing attackers to inject malicious patterns into the model."
      remediation: "Validate and sanitize all training data. Use checksums and signatures. Filter malicious patterns. Only use trusted data sources. Implement data provenance tracking."
      examples:
        vulnerable: "samples/llm03_training_data_poisoning.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://arxiv.org/abs/2004.14160"
    paths:
      include:
        - "*.py"

  # ----------------------------------------------------------------------------
  # LLM04: Model Denial of Service
  # ----------------------------------------------------------------------------
  # Description: Resource exhaustion attacks through excessive token usage,
  #              long prompts, or rapid API calls
  # CWE: CWE-400, CWE-770
  # ----------------------------------------------------------------------------
  - id: llm04-dos-no-rate-limiting
    patterns:
      - pattern-either:
          - pattern: openai.ChatCompletion.create(...)
          - pattern: openai.Completion.create(...)
          - pattern: $CLIENT.chat.completions.create(...)
          - pattern: $CLIENT.messages.create(...)
      - pattern-inside: |
          def $FUNC(...):
            ...
      - pattern-not-inside: |
          @rate_limit(...)
          def $FUNC(...):
            ...
      - pattern-not-inside: |
          if rate_limit_check():
            ...
    message: "LLM04: Model Denial of Service - LLM API call without rate limiting protection"
    severity: WARNING
    languages: [python]
    metadata:
      category: security
      subcategory: denial-of-service
      owasp: "LLM04"
      owasp-title: "Model Denial of Service"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM04.html"
      cwe: ["CWE-400", "CWE-770"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/400.html", "https://cwe.mitre.org/data/definitions/770.html"]
      tags: ["owasp", "llm04", "dos", "denial-of-service", "rate-limiting", "resource-exhaustion", "llm", "ai-security"]
      technology: ["python", "openai", "anthropic", "llm"]
      confidence: "high"
      impact: "medium"
      likelihood: "high"
      description: "LLM API calls are made without rate limiting protection, allowing attackers to exhaust resources through rapid or excessive API calls."
      remediation: "Implement rate limiting, token limits, and request throttling for LLM API calls. Use circuit breakers and request queuing. Monitor API usage and costs."
      examples:
        vulnerable: "samples/llm04_model_dos.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://owasp.org/www-community/attacks/Denial_of_Service"
    paths:
      include:
        - "*.py"

  - id: llm04-dos-no-token-limit
    patterns:
      - pattern-either:
          - pattern: openai.ChatCompletion.create(messages=$MESSAGES, ...)
          - pattern: openai.Completion.create(prompt=$PROMPT, ...)
          - pattern: $CLIENT.chat.completions.create(messages=$MESSAGES, ...)
          - pattern: $CLIENT.messages.create(messages=$MESSAGES, ...)
      - pattern-not: |
          openai.ChatCompletion.create(..., max_tokens=$LIMIT, ...)
      - pattern-not: |
          openai.Completion.create(..., max_tokens=$LIMIT, ...)
      - pattern-not: |
          $CLIENT.chat.completions.create(..., max_tokens=$LIMIT, ...)
    message: "LLM04: Model Denial of Service - LLM API call without max_tokens limit"
    severity: WARNING
    languages: [python]
    metadata:
      category: security
      subcategory: denial-of-service
      owasp: "LLM04"
      owasp-title: "Model Denial of Service"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM04.html"
      cwe: ["CWE-400", "CWE-770"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/400.html", "https://cwe.mitre.org/data/definitions/770.html"]
      tags: ["owasp", "llm04", "dos", "denial-of-service", "token-limit", "resource-exhaustion", "llm", "ai-security"]
      technology: ["python", "openai", "anthropic", "llm"]
      confidence: "high"
      impact: "medium"
      likelihood: "high"
      description: "LLM API calls are made without max_tokens limit, allowing attackers to cause resource exhaustion through excessive token generation."
      remediation: "Always set max_tokens to limit response size and prevent resource exhaustion. Set appropriate limits based on use case and cost constraints."
      examples:
        vulnerable: "samples/llm04_model_dos.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://owasp.org/www-community/attacks/Denial_of_Service"
    paths:
      include:
        - "*.py"

  # ----------------------------------------------------------------------------
  # LLM05: Supply Chain Vulnerabilities
  # ----------------------------------------------------------------------------
  # Description: Using untrusted or unverified models, libraries, or plugins
  #              from external sources
  # CWE: CWE-494, CWE-502
  # ----------------------------------------------------------------------------
  - id: llm05-supply-chain-untrusted-model
    patterns:
      - pattern-either:
          - pattern: |
              $MODEL = load_model($URL)
          - pattern: |
              $MODEL = download_model($URL)
          - pattern: |
              $MODEL = from_pretrained($URL)
      - pattern-not-inside: |
          $CHECKSUM = verify_checksum($URL)
          ...
      - pattern-not-inside: |
          $SIGNATURE = verify_signature($URL)
          ...
      - metavariable-regex:
          metavariable: $URL
          regex: (http://|https://)
    message: "LLM05: Supply Chain Vulnerabilities - Model loaded from untrusted source without verification"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: supply-chain
      owasp: "LLM05"
      owasp-title: "Supply Chain Vulnerabilities"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM05.html"
      cwe: ["CWE-494", "CWE-502"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/494.html", "https://cwe.mitre.org/data/definitions/502.html"]
      tags: ["owasp", "llm05", "supply-chain", "untrusted-model", "model-verification", "llm", "ai-security"]
      technology: ["python", "ml", "llm", "huggingface", "pytorch", "tensorflow"]
      confidence: "high"
      impact: "critical"
      likelihood: "medium"
      description: "Models are loaded from untrusted URLs or sources without checksum verification, signature validation, or integrity checks, allowing attackers to supply malicious models."
      remediation: "Verify model checksums and signatures. Use trusted sources. Implement supply chain security. Use signed model artifacts. Verify model provenance."
      examples:
        vulnerable: "samples/llm05_supply_chain.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://owasp.org/www-project-dependency-check/"
    paths:
      include:
        - "*.py"

  - id: llm05-supply-chain-unverified-plugin
    patterns:
      - pattern-either:
          - pattern: |
              $PLUGIN = importlib.import_module($MODULE_NAME)
          - pattern: |
              $PLUGIN = __import__($MODULE_NAME)
          - pattern: |
              exec($CODE)
          - pattern: |
              eval($CODE)
      - pattern-inside: |
          $RESPONSE = openai.ChatCompletion.create(...)
          ...
          $PLUGIN_CODE = $RESPONSE.choices[0].message.content
          ...
    message: "LLM05: Supply Chain Vulnerabilities - Plugin code loaded from LLM output without verification"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: supply-chain
      owasp: "LLM05"
      owasp-title: "Supply Chain Vulnerabilities"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM05.html"
      cwe: ["CWE-502", "CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/502.html", "https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["owasp", "llm05", "supply-chain", "plugin", "code-execution", "llm", "ai-security"]
      technology: ["python", "openai", "anthropic", "llm"]
      confidence: "high"
      impact: "critical"
      likelihood: "medium"
      description: "Plugin code is loaded and executed from LLM output without verification, allowing attackers to inject malicious code through the LLM."
      remediation: "Never load or execute code from LLM output. Use a whitelist of trusted plugins only. Implement code signing and verification for plugins."
      examples:
        vulnerable: "samples/llm05_supply_chain.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://owasp.org/www-community/vulnerabilities/Code_Injection"
    paths:
      include:
        - "*.py"

  # ----------------------------------------------------------------------------
  # LLM06: Sensitive Information Disclosure
  # ----------------------------------------------------------------------------
  # Description: Sensitive data (secrets, PII, credentials) included in
  #              prompts or exposed in LLM responses
  # CWE: CWE-200, CWE-312, CWE-359
  # ----------------------------------------------------------------------------
  - id: llm06-sensitive-info-in-prompt
    patterns:
      - pattern-either:
          - pattern: |
              openai.ChatCompletion.create(messages=[..., {"content": $PROMPT}, ...])
          - pattern: |
              openai.Completion.create(prompt=$PROMPT, ...)
          - pattern: |
              $CLIENT.chat.completions.create(messages=[..., {"content": $PROMPT}, ...])
      - metavariable-regex:
          metavariable: $PROMPT
          regex: (password|secret|api_key|token|credential|private_key|ssn|social_security|credit_card|pii|personally_identifiable)
    message: "LLM06: Sensitive Information Disclosure - Potential sensitive data included in LLM prompt"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: information-disclosure
      owasp: "LLM06"
      owasp-title: "Sensitive Information Disclosure"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM06.html"
      cwe: ["CWE-200", "CWE-312", "CWE-359"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/200.html", "https://cwe.mitre.org/data/definitions/312.html", "https://cwe.mitre.org/data/definitions/359.html"]
      tags: ["owasp", "llm06", "sensitive-info", "secrets", "pii", "credentials", "llm", "ai-security", "data-leakage"]
      technology: ["python", "openai", "anthropic", "llm"]
      confidence: "medium"
      impact: "high"
      likelihood: "medium"
      description: "Sensitive data (secrets, API keys, passwords, PII, credentials) is included in LLM prompts, potentially exposing them to the LLM provider or in logs."
      remediation: "Never include secrets, credentials, or PII in LLM prompts. Use environment variables or secure vaults. Implement data classification and masking."
      examples:
        vulnerable: "samples/llm06_sensitive_info.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://owasp.org/www-community/vulnerabilities/Information_exposure"
    paths:
      include:
        - "*.py"

  - id: llm06-sensitive-info-in-response-logging
    patterns:
      - pattern-either:
          - pattern: |
              $RESPONSE = openai.ChatCompletion.create(...)
          - pattern: |
              $RESPONSE = openai.Completion.create(...)
          - pattern: |
              $RESPONSE = $CLIENT.chat.completions.create(...)
      - pattern-inside: |
          $CONTENT = $RESPONSE.choices[0].message.content
          ...
          print($CONTENT)
          ...
      - pattern-inside: |
          $CONTENT = $RESPONSE.choices[0].message.content
          ...
          logger.info($CONTENT)
          ...
    message: "LLM06: Sensitive Information Disclosure - LLM response logged without sanitization"
    severity: WARNING
    languages: [python]
    metadata:
      category: security
      subcategory: information-disclosure
      owasp: "LLM06"
      owasp-title: "Sensitive Information Disclosure"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM06.html"
      cwe: ["CWE-200", "CWE-312", "CWE-359"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/200.html", "https://cwe.mitre.org/data/definitions/312.html", "https://cwe.mitre.org/data/definitions/359.html"]
      tags: ["owasp", "llm06", "sensitive-info", "logging", "pii", "llm", "ai-security", "data-leakage"]
      technology: ["python", "openai", "anthropic", "llm"]
      confidence: "high"
      impact: "medium"
      likelihood: "high"
      description: "LLM responses are logged or printed without sanitization, potentially exposing sensitive information that the LLM may have included in its output."
      remediation: "Sanitize LLM responses before logging. Remove or redact sensitive information. Use structured logging with data classification."
      examples:
        vulnerable: "samples/llm06_sensitive_info.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://owasp.org/www-community/vulnerabilities/Information_exposure"
    paths:
      include:
        - "*.py"

  # ----------------------------------------------------------------------------
  # LLM07: Insecure Plugin Design
  # ----------------------------------------------------------------------------
  # Description: Plugins or tools executed by LLM without proper authorization,
  #              validation, or sandboxing
  # CWE: CWE-284, CWE-306
  # ----------------------------------------------------------------------------
  - id: llm07-insecure-plugin-execution
    patterns:
      - pattern-either:
          - pattern: |
              $RESPONSE = openai.ChatCompletion.create(...)
          - pattern: |
              $RESPONSE = openai.Completion.create(...)
          - pattern: |
              $RESPONSE = $CLIENT.chat.completions.create(...)
      - pattern-inside: |
          $ACTION = $RESPONSE.choices[0].message.content
          ...
          $FUNC($ACTION)
          ...
      - pattern-not-inside: |
          if is_authorized($ACTION):
            ...
      - pattern-not-inside: |
          if validate_action($ACTION):
            ...
    message: "LLM07: Insecure Plugin Design - LLM output used to execute actions without authorization check"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: authorization
      owasp: "LLM07"
      owasp-title: "Insecure Plugin Design"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM07.html"
      cwe: ["CWE-284", "CWE-306"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/284.html", "https://cwe.mitre.org/data/definitions/306.html"]
      tags: ["owasp", "llm07", "plugin", "authorization", "access-control", "llm", "ai-security"]
      technology: ["python", "openai", "anthropic", "llm"]
      confidence: "high"
      impact: "critical"
      likelihood: "medium"
      description: "LLM output is used to execute actions or call functions without authorization checks, allowing the LLM to perform unauthorized operations."
      remediation: "Always validate and authorize LLM-requested actions. Use whitelists and sandboxing. Implement principle of least privilege for LLM-driven actions."
      examples:
        vulnerable: "samples/llm07_insecure_plugin.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://owasp.org/www-community/vulnerabilities/Improper_Authorization"
    paths:
      include:
        - "*.py"

  - id: llm07-insecure-plugin-file-access
    patterns:
      - pattern-either:
          - pattern: |
              $RESPONSE = openai.ChatCompletion.create(...)
          - pattern: |
              $RESPONSE = openai.Completion.create(...)
      - pattern-inside: |
          $FILENAME = $RESPONSE.choices[0].message.content
          ...
          open($FILENAME, ...)
          ...
      - pattern-not-inside: |
          if is_safe_path($FILENAME):
            ...
    message: "LLM07: Insecure Plugin Design - LLM output used for file operations without path validation"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: path-traversal
      owasp: "LLM07"
      owasp-title: "Insecure Plugin Design"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM07.html"
      cwe: ["CWE-22", "CWE-284"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/22.html", "https://cwe.mitre.org/data/definitions/284.html"]
      tags: ["owasp", "llm07", "plugin", "path-traversal", "file-access", "llm", "ai-security"]
      technology: ["python", "openai", "anthropic", "llm"]
      confidence: "high"
      impact: "high"
      likelihood: "medium"
      description: "LLM output is used for file operations without path validation, allowing directory traversal attacks and unauthorized file access."
      remediation: "Validate and sanitize file paths from LLM output. Use path whitelists and prevent directory traversal. Use os.path.abspath() and validate against allowed directories."
      examples:
        vulnerable: "samples/llm07_insecure_plugin.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://owasp.org/www-community/attacks/Path_Traversal"
    paths:
      include:
        - "*.py"

  # ----------------------------------------------------------------------------
  # LLM08: Excessive Agency
  # ----------------------------------------------------------------------------
  # Description: LLM granted excessive permissions or autonomy to perform
  #              dangerous operations
  # CWE: CWE-250, CWE-284
  # ----------------------------------------------------------------------------
  - id: llm08-excessive-agency-system-commands
    patterns:
      - pattern-either:
          - pattern: |
              $RESPONSE = openai.ChatCompletion.create(...)
          - pattern: |
              $RESPONSE = openai.Completion.create(...)
          - pattern: |
              $RESPONSE = $CLIENT.chat.completions.create(...)
      - pattern-inside: |
          $COMMAND = $RESPONSE.choices[0].message.content
          ...
          os.system($COMMAND)
          ...
      - pattern-not-inside: |
          if is_whitelisted_command($COMMAND):
            ...
    message: "LLM08: Excessive Agency - LLM output used to execute system commands without restrictions"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: excessive-privileges
      owasp: "LLM08"
      owasp-title: "Excessive Agency"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM08.html"
      cwe: ["CWE-250", "CWE-284"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/250.html", "https://cwe.mitre.org/data/definitions/284.html"]
      tags: ["owasp", "llm08", "excessive-agency", "privilege-escalation", "system-commands", "llm", "ai-security"]
      technology: ["python", "openai", "anthropic", "llm"]
      confidence: "high"
      impact: "critical"
      likelihood: "medium"
      description: "LLM output is used to execute system commands without restrictions or whitelisting, granting the LLM excessive permissions to perform dangerous operations."
      remediation: "Restrict LLM agency. Use command whitelists and require human approval for dangerous operations. Implement principle of least privilege."
      examples:
        vulnerable: "samples/llm08_excessive_agency.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://owasp.org/www-community/vulnerabilities/Improper_Privilege_Management"
    paths:
      include:
        - "*.py"

  - id: llm08-excessive-agency-database-write
    patterns:
      - pattern-either:
          - pattern: |
              $RESPONSE = openai.ChatCompletion.create(...)
          - pattern: |
              $RESPONSE = openai.Completion.create(...)
      - pattern-inside: |
          $QUERY = $RESPONSE.choices[0].message.content
          ...
          db.execute($QUERY)
          ...
      - pattern-not-inside: |
          if is_readonly_query($QUERY):
            ...
    message: "LLM08: Excessive Agency - LLM output used for database write operations without restrictions"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: excessive-privileges
      owasp: "LLM08"
      owasp-title: "Excessive Agency"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM08.html"
      cwe: ["CWE-250", "CWE-284"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/250.html", "https://cwe.mitre.org/data/definitions/284.html"]
      tags: ["owasp", "llm08", "excessive-agency", "database", "write-operations", "llm", "ai-security"]
      technology: ["python", "openai", "anthropic", "llm", "database"]
      confidence: "high"
      impact: "critical"
      likelihood: "medium"
      description: "LLM output is used for database write operations (INSERT, UPDATE, DELETE) without restrictions, allowing the LLM to modify or delete data."
      remediation: "Limit LLM to read-only operations or require explicit authorization for writes. Use parameterized queries and validate all database operations."
      examples:
        vulnerable: "samples/llm08_excessive_agency.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://owasp.org/www-community/vulnerabilities/Improper_Privilege_Management"
    paths:
      include:
        - "*.py"

  # ----------------------------------------------------------------------------
  # LLM09: Overreliance
  # ----------------------------------------------------------------------------
  # Description: Blindly trusting LLM output without validation, verification,
  #              or human oversight
  # CWE: CWE-345, CWE-754
  # ----------------------------------------------------------------------------
  - id: llm09-overreliance-no-validation
    patterns:
      - pattern-either:
          - pattern: |
              $RESPONSE = openai.ChatCompletion.create(...)
          - pattern: |
              $RESPONSE = openai.Completion.create(...)
          - pattern: |
              $RESPONSE = $CLIENT.chat.completions.create(...)
      - pattern-inside: |
          $RESULT = $RESPONSE.choices[0].message.content
          ...
          return $RESULT
          ...
      - pattern-not-inside: |
          $VALIDATED = validate($RESULT)
          ...
      - pattern-not-inside: |
          if verify($RESULT):
            ...
    message: "LLM09: Overreliance - LLM output returned without validation or verification"
    severity: WARNING
    languages: [python]
    metadata:
      category: security
      subcategory: validation
      owasp: "LLM09"
      owasp-title: "Overreliance"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM09.html"
      cwe: ["CWE-345", "CWE-754"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/345.html", "https://cwe.mitre.org/data/definitions/754.html"]
      tags: ["owasp", "llm09", "overreliance", "validation", "trust", "llm", "ai-security"]
      technology: ["python", "openai", "anthropic", "llm"]
      confidence: "high"
      impact: "high"
      likelihood: "high"
      description: "LLM output is returned or used without validation or verification, leading to potential errors, hallucinations, or incorrect results being used in production."
      remediation: "Always validate and verify LLM outputs. Implement human-in-the-loop for critical decisions. Use schema validation for structured outputs. Implement confidence scoring."
      examples:
        vulnerable: "samples/llm09_overreliance.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://owasp.org/www-community/vulnerabilities/Improper_Input_Validation"
    paths:
      include:
        - "*.py"

  - id: llm09-overreliance-critical-decision
    patterns:
      - pattern-either:
          - pattern: |
              $RESPONSE = openai.ChatCompletion.create(...)
          - pattern: |
              $RESPONSE = openai.Completion.create(...)
      - pattern-inside: |
          $DECISION = $RESPONSE.choices[0].message.content
          ...
          if $DECISION == "approve":
            approve_transaction(...)
            ...
      - pattern-not-inside: |
          if human_approval_required():
            ...
    message: "LLM09: Overreliance - Critical decision made based solely on LLM output without human oversight"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: validation
      owasp: "LLM09"
      owasp-title: "Overreliance"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM09.html"
      cwe: ["CWE-754", "CWE-345"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/754.html", "https://cwe.mitre.org/data/definitions/345.html"]
      tags: ["owasp", "llm09", "overreliance", "critical-decision", "automation", "llm", "ai-security"]
      technology: ["python", "openai", "anthropic", "llm"]
      confidence: "high"
      impact: "critical"
      likelihood: "medium"
      description: "Critical decisions (financial transactions, approvals, medical diagnosis) are made based solely on LLM output without human oversight or verification."
      remediation: "Require human approval for critical decisions. Never automate high-risk actions based solely on LLM output. Implement multi-factor verification for sensitive operations."
      examples:
        vulnerable: "samples/llm09_overreliance.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://owasp.org/www-community/vulnerabilities/Improper_Error_Handling"
    paths:
      include:
        - "*.py"

  # ----------------------------------------------------------------------------
  # LLM10: Model Theft
  # ----------------------------------------------------------------------------
  # Description: Unauthorized access, copying, or extraction of proprietary
  #              models or training data
  # CWE: CWE-200, CWE-497
  # ----------------------------------------------------------------------------
  - id: llm10-model-theft-exposed-endpoint
    patterns:
      - pattern-either:
          - pattern: |
              @app.route("/model", methods=["GET"])
          - pattern: |
              @app.route("/download", methods=["GET"])
          - pattern: |
              @app.route("/weights", methods=["GET"])
      - pattern-inside: |
          def $FUNC(...):
            return $MODEL_FILE
            ...
      - pattern-not-inside: |
          if is_authorized():
            ...
    message: "LLM10: Model Theft - Model file exposed via unauthenticated endpoint"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: access-control
      owasp: "LLM10"
      owasp-title: "Model Theft"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM10.html"
      cwe: ["CWE-200", "CWE-497"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/200.html", "https://cwe.mitre.org/data/definitions/497.html"]
      tags: ["owasp", "llm10", "model-theft", "intellectual-property", "access-control", "llm", "ai-security"]
      technology: ["python", "flask", "django", "ml", "llm"]
      confidence: "high"
      impact: "high"
      likelihood: "medium"
      description: "Model files, weights, or checkpoints are exposed via unauthenticated endpoints, allowing unauthorized access and potential model theft or extraction."
      remediation: "Protect model endpoints with authentication and authorization. Limit access to authorized users only. Use encryption for model storage. Implement access logging."
      examples:
        vulnerable: "samples/llm10_model_theft.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://owasp.org/www-community/vulnerabilities/Improper_Access_Control"
    paths:
      include:
        - "*.py"

  - id: llm10-model-theft-no-access-control
    patterns:
      - pattern-either:
          - pattern: |
              $MODEL = load_model($PATH)
          - pattern: |
              $MODEL = pickle.load($FILE)
          - pattern: |
              $MODEL = torch.load($FILE)
      - pattern-inside: |
          def $FUNC(...):
            ...
      - pattern-not-inside: |
          if check_permissions():
            ...
    message: "LLM10: Model Theft - Model loading without access control checks"
    severity: WARNING
    languages: [python]
    metadata:
      category: security
      subcategory: access-control
      owasp: "LLM10"
      owasp-title: "Model Theft"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM10.html"
      cwe: ["CWE-497", "CWE-200"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/497.html", "https://cwe.mitre.org/data/definitions/200.html"]
      tags: ["owasp", "llm10", "model-theft", "intellectual-property", "access-control", "llm", "ai-security"]
      technology: ["python", "ml", "llm", "pytorch", "tensorflow"]
      confidence: "medium"
      impact: "high"
      likelihood: "low"
      description: "Model loading operations are performed without access control checks, allowing unauthorized users to load and potentially extract proprietary models."
      remediation: "Implement access controls for model loading. Log all model access attempts. Use file system permissions. Encrypt model files at rest."
      examples:
        vulnerable: "samples/llm10_model_theft.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://owasp.org/www-community/vulnerabilities/Improper_Access_Control"
    paths:
      include:
        - "*.py"
