rules:
  # ============================================================================
  # LLM01: Prompt Injection - Anthropic Generic
  # ============================================================================
  # Detects unsanitized user input in Anthropic prompts
  # OWASP: LLM01
  # CWE: CWE-79, CWE-94
  # ============================================================================

  - id: anthropic-prompt-injection-direct
    patterns:
      - pattern-either:
          - pattern: |
              $CLIENT.messages.create(messages=[..., {"role": "user", "content": $USER_INPUT}, ...])
          - pattern: |
              $CLIENT.messages.create(messages=[..., {"content": $USER_INPUT}, ...])
      # Only check for well-known standard library/framework escaping functions
      # Note: Custom sanitization functions may not be detected, leading to potential false positives
      - pattern-not-inside: |
          $SANITIZED = html.escape($USER_INPUT)
          ...
      - pattern-not-inside: |
          $SANITIZED = markupsafe.escape($USER_INPUT)
          ...
      - pattern-not-inside: |
          $SANITIZED = bleach.clean($USER_INPUT)
          ...
      - metavariable-regex:
          metavariable: $USER_INPUT
          regex: (request\.|input\(|sys\.argv|getenv|environ|args|kwargs|params|query|body|form|json|data|user_input|user_input_text|user_message|user_prompt|prompt_text|raw_input|raw_prompt)
    message: "LLM01: Prompt Injection - User input directly inserted into Anthropic prompt without sanitization"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: injection
      owasp: "LLM01"
      owasp-title: "Prompt Injection"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM01.html"
      cwe: ["CWE-79", "CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/79.html", "https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["owasp", "llm01", "prompt-injection", "injection", "input-validation", "llm", "ai-security", "anthropic"]
      technology: ["python", "anthropic", "llm"]
      confidence: "high"
      impact: "critical"
      likelihood: "high"
      description: "User input is directly inserted into Anthropic prompts without sanitization, allowing attackers to manipulate LLM behavior, bypass system prompts, or inject malicious instructions."
      remediation: "Sanitize and validate all user input before including in prompts. Use prompt templates with parameterized inputs. Implement input validation, output encoding, and prompt engineering best practices."
      examples:
        vulnerable: "samples/llm01_prompt_injection.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://learnprompting.org/docs/category/-prompt-injection"
    paths:
      include:
        - "*.py"

  - id: anthropic-prompt-injection-concatenation
    patterns:
      - pattern-either:
          - pattern: |
              $CLIENT.messages.create(messages=[..., {"role": "user", "content": $PROMPT + $USER_INPUT}, ...])
          - pattern: |
              $CLIENT.messages.create(messages=[..., {"content": $PROMPT + $USER_INPUT}, ...])
          - pattern: $PROMPT = ... + $USER_INPUT + ...
          - pattern: f"...{$USER_INPUT}..."
      - pattern-inside: |
          $RESPONSE = $CLIENT.messages.create(...)
          ...
      - metavariable-regex:
          metavariable: $USER_INPUT
          regex: (request\.|input\(|sys\.argv|getenv|environ|args|kwargs|params|query|body|form|json|data|user_input|user_input_text|user_message|user_prompt|prompt_text|raw_input|raw_prompt)
    message: "LLM01: Prompt Injection - User input concatenated into Anthropic prompt without sanitization"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: injection
      owasp: "LLM01"
      owasp-title: "Prompt Injection"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM01.html"
      cwe: ["CWE-79", "CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/79.html", "https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["owasp", "llm01", "prompt-injection", "injection", "input-validation", "llm", "ai-security", "anthropic", "string-concatenation"]
      technology: ["python", "anthropic", "llm"]
      confidence: "medium"
      impact: "critical"
      likelihood: "high"
      description: "User input is concatenated into Anthropic prompts using string operations (+, f-strings) without sanitization, making prompt injection attacks easier."
      remediation: "Use parameterized prompt templates instead of string concatenation. Validate and sanitize all user inputs. Avoid f-strings or + operators with user input in prompts."
      examples:
        vulnerable: "samples/llm01_prompt_injection.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://learnprompting.org/docs/category/-prompt-injection"
    paths:
      include:
        - "*.py"
