rules:
  # ============================================================================
  # LLM01: Prompt Injection - Anthropic Generic (AI-Enhanced Detection)
  # ============================================================================
  # This rule set is designed to work with AI analysis to reduce false positives.
  # Semgrep catches potential patterns broadly, AI determines exploitability.
  # 
  # Usage with AI:
  #   python -m llm_scan.runner . --enable-ai-filter \
  #     --ai-analyze-rules anthropic-llm01-prompt-injection-direct \
  #     --ai-analyze-rules anthropic-llm01-prompt-injection-concatenation \
  #     --ai-analyze-rules anthropic-llm01-prompt-injection-indirect
  #
  # OWASP: LLM01
  # CWE: CWE-79, CWE-94
  # ============================================================================

  - id: anthropic-llm01-prompt-injection-direct
    patterns:
      - pattern-either:
          # Direct user input in user messages
          - pattern: |
              anthropic.Anthropic().messages.create(messages=[..., {"role": "user", "content": $USER_INPUT}, ...])
          - pattern: |
              anthropic.Anthropic().messages.create(messages=[..., {"content": $USER_INPUT}, ...])
          - pattern: |
              $CLIENT.messages.create(messages=[..., {"role": "user", "content": $USER_INPUT}, ...])
          - pattern: |
              $CLIENT.messages.create(messages=[..., {"content": $USER_INPUT}, ...])
      # Only exclude well-known standard library/framework escaping functions
      # AI will analyze if custom sanitization is sufficient
      - pattern-not-inside: |
          $SANITIZED = html.escape($USER_INPUT)
          ...
      - pattern-not-inside: |
          $SANITIZED = markupsafe.escape($USER_INPUT)
          ...
      - pattern-not-inside: |
          $SANITIZED = bleach.clean($USER_INPUT)
          ...
      - metavariable-regex:
          metavariable: $USER_INPUT
          regex: (request\.|input\(|sys\.argv|getenv|environ|args|kwargs|params|query|body|form|json|data|user_input|user_input_text|user_message|user_prompt|prompt_text|raw_input|raw_prompt|user_data|user_content)
    message: "LLM01: Potential Prompt Injection - User input directly inserted into Anthropic prompt"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: injection
      owasp: "LLM01"
      owasp-title: "Prompt Injection"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM01.html"
      cwe: ["CWE-79", "CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/79.html", "https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["owasp", "llm01", "prompt-injection", "injection", "input-validation", "llm", "ai-security", "anthropic", "ai-enhanced"]
      technology: ["python", "anthropic", "llm"]
      confidence: "medium"  # Medium confidence - AI will analyze to determine if exploitable
      impact: "critical"
      likelihood: "medium"  # AI will assess actual likelihood based on context
      description: "User input is directly inserted into Anthropic prompts. This pattern may be vulnerable to prompt injection if input is not properly sanitized or validated. AI analysis will determine if the vulnerability is actually exploitable based on code context, sanitization, and validation."
      remediation: "Sanitize and validate all user input before including in prompts. Use prompt templates with parameterized inputs. Implement input validation, output encoding, and prompt engineering best practices. Consider using delimiters to separate user data from instructions."
      ai_analysis_recommended: true
      examples:
        vulnerable: "samples/llm01_prompt_injection.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://learnprompting.org/docs/category/-prompt-injection"
    paths:
      include:
        - "*.py"

  - id: anthropic-llm01-prompt-injection-concatenation
    patterns:
      - pattern-either:
          # String concatenation in prompts
          - pattern: |
              anthropic.Anthropic().messages.create(messages=[..., {"role": "user", "content": $PROMPT + $USER_INPUT}, ...])
          - pattern: |
              anthropic.Anthropic().messages.create(messages=[..., {"content": $PROMPT + $USER_INPUT}, ...])
          - pattern: |
              $CLIENT.messages.create(messages=[..., {"role": "user", "content": $PROMPT + $USER_INPUT}, ...])
          - pattern: |
              $CLIENT.messages.create(messages=[..., {"content": $PROMPT + $USER_INPUT}, ...])
          # F-string interpolation
          - pattern: |
              anthropic.Anthropic().messages.create(messages=[..., {"role": "user", "content": f"...{$USER_INPUT}..."}, ...])
          - pattern: |
              anthropic.Anthropic().messages.create(messages=[..., {"content": f"...{$USER_INPUT}..."}, ...])
          - pattern: |
              $CLIENT.messages.create(messages=[..., {"role": "user", "content": f"...{$USER_INPUT}..."}, ...])
          - pattern: |
              $CLIENT.messages.create(messages=[..., {"content": f"...{$USER_INPUT}..."}, ...])
          # Variable assignment before API call
          - pattern: |
              $PROMPT = ... + $USER_INPUT + ...
              ...
              anthropic.Anthropic().messages.create(messages=[..., {"content": $PROMPT}, ...])
          - pattern: |
              $PROMPT = f"...{$USER_INPUT}..."
              ...
              anthropic.Anthropic().messages.create(messages=[..., {"content": $PROMPT}, ...])
      - pattern-inside: |
          $RESPONSE = anthropic.Anthropic().messages.create(...)
          ...
      - metavariable-regex:
          metavariable: $USER_INPUT
          regex: (request\.|input\(|sys\.argv|getenv|environ|args|kwargs|params|query|body|form|json|data|user_input|user_input_text|user_message|user_prompt|prompt_text|raw_input|raw_prompt|user_data|user_content)
    message: "LLM01: Potential Prompt Injection - User input concatenated into Anthropic prompt"
    severity: ERROR
    languages: [python]
    metadata:
      category: security
      subcategory: injection
      owasp: "LLM01"
      owasp-title: "Prompt Injection"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM01.html"
      cwe: ["CWE-79", "CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/79.html", "https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["owasp", "llm01", "prompt-injection", "injection", "input-validation", "llm", "ai-security", "anthropic", "string-concatenation", "ai-enhanced"]
      technology: ["python", "anthropic", "llm"]
      confidence: "medium"  # Medium confidence - AI will analyze context
      impact: "critical"
      likelihood: "medium"  # AI will assess actual likelihood
      description: "User input is concatenated into Anthropic prompts using string operations (+, f-strings, .format()). This pattern is vulnerable to prompt injection if input is not properly sanitized. AI analysis will determine exploitability based on sanitization, validation, and code context."
      remediation: "Use parameterized prompt templates instead of string concatenation. Validate and sanitize all user inputs. Avoid f-strings or + operators with user input in prompts. Use structured message formats with clear separation between instructions and data."
      ai_analysis_recommended: true
      examples:
        vulnerable: "samples/llm01_prompt_injection.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://learnprompting.org/docs/category/-prompt-injection"
    paths:
      include:
        - "*.py"

  - id: anthropic-llm01-prompt-injection-indirect
    mode: taint
    pattern-sources:
      # Flask request data
      - pattern: flask.request.args.get(...)
      - pattern: flask.request.form.get(...)
      - pattern: flask.request.get_json(...)
      - pattern: flask.request.json.get(...)
      - pattern: request.args.get(...)
      - pattern: request.form.get(...)
      - pattern: request.get_json(...)
      # Django request data
      - pattern: request.GET.get(...)
      - pattern: request.POST.get(...)
      - pattern: request.body
      # External requests (fetching URL content that might contain instructions)
      - pattern: requests.get(...).text
      - pattern: requests.get(...).content
      - pattern: requests.get(...).json()
      - pattern: urllib.request.urlopen(...).read()
      # File reads (potentially untrusted files)
      - pattern: open(..., "r").read()
      - pattern: Path(...).read_text()
      # Environment variables (if user-controlled)
      - pattern: os.getenv(...)
      - pattern: os.environ.get(...)
      # Database queries (if user-controlled data)
      - pattern: $CURSOR.fetchone()
      - pattern: $CURSOR.fetchall()
    pattern-sinks:
      - pattern: anthropic.Anthropic().messages.create(..., messages=...)
      - pattern: $CLIENT.messages.create(..., messages=...)
    message: "LLM01: Potential Indirect Prompt Injection - Untrusted data flows into Anthropic prompt"
    severity: WARNING
    languages: [python]
    metadata:
      category: security
      subcategory: injection
      owasp: "LLM01"
      owasp-title: "Prompt Injection (Indirect)"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM01.html"
      cwe: ["CWE-79", "CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/79.html", "https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["owasp", "llm01", "prompt-injection", "indirect-injection", "injection", "llm", "ai-security", "anthropic", "taint", "ai-enhanced"]
      technology: ["python", "anthropic", "llm"]
      confidence: "medium"  # Medium confidence - AI will analyze if data is actually untrusted/exploitable
      impact: "high"
      likelihood: "medium"  # AI will assess based on data source trustworthiness
      description: "Data from potentially untrusted sources (web requests, external APIs, files, databases) flows into Anthropic prompts. This enables Indirect Prompt Injection where external content contains hidden instructions. AI analysis will determine if the data source is actually untrusted and if the vulnerability is exploitable."
      remediation: "Sanitize and validate all external content before including in prompts. Use delimiters to separate data from instructions. Treat all external content as potentially malicious. Implement content filtering and validation for external data sources."
      ai_analysis_recommended: true
      examples:
        vulnerable: "samples/llm01_prompt_injection.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://learnprompting.org/docs/category/-prompt-injection"
    paths:
      include:
        - "*.py"

  - id: anthropic-llm01-prompt-injection-template
    patterns:
      - pattern-either:
          # Template-like patterns (e.g., using .format(), % formatting)
          - pattern: |
              anthropic.Anthropic().messages.create(messages=[..., {"content": $TEMPLATE.format(...)}, ...])
          - pattern: |
              anthropic.Anthropic().messages.create(messages=[..., {"content": $TEMPLATE % ...}, ...])
          - pattern: |
              $CLIENT.messages.create(messages=[..., {"content": $TEMPLATE.format(...)}, ...])
          - pattern: |
              $CLIENT.messages.create(messages=[..., {"content": $TEMPLATE % ...}, ...])
      # Check if template contains user input patterns
      - pattern-inside: |
          $TEMPLATE = ...
          ...
          anthropic.Anthropic().messages.create(...)
      - metavariable-regex:
          metavariable: $TEMPLATE
          regex: (.*\{.*\}.*|.*%.*)
    message: "LLM01: Potential Prompt Injection - Template formatting with user input in Anthropic prompt"
    severity: WARNING
    languages: [python]
    metadata:
      category: security
      subcategory: injection
      owasp: "LLM01"
      owasp-title: "Prompt Injection"
      owasp-url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/LLM_Top_10/LLM01.html"
      cwe: ["CWE-79", "CWE-94"]
      cwe-url: ["https://cwe.mitre.org/data/definitions/79.html", "https://cwe.mitre.org/data/definitions/94.html"]
      tags: ["owasp", "llm01", "prompt-injection", "injection", "template", "llm", "ai-security", "anthropic", "ai-enhanced"]
      technology: ["python", "anthropic", "llm"]
      confidence: "medium"  # Medium confidence - AI will verify if user input is actually in template
      impact: "high"
      likelihood: "medium"  # AI will assess based on template content
      description: "Template formatting (.format(), % operator) is used in Anthropic prompts. If user input is included in the template, this can lead to prompt injection. AI analysis will determine if user input is actually present and if the vulnerability is exploitable."
      remediation: "Avoid including user input in prompt templates. Use parameterized templates with validated placeholders. Sanitize any dynamic content before template substitution."
      ai_analysis_recommended: true
      examples:
        vulnerable: "samples/llm01_prompt_injection.py"
      references:
        - "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
        - "https://learnprompting.org/docs/category/-prompt-injection"
    paths:
      include:
        - "*.py"
