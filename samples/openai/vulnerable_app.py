"""
Sample vulnerable application demonstrating LLM security issues.
This file contains multiple vulnerabilities that should be detected by the scanner.
"""

import openai
import subprocess
import os
from anthropic import Anthropic


def process_user_query(user_input: str):
    """Process user query using LLM - VULNERABLE."""
    # VULNERABILITY: LLM output passed to eval
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": f"Generate Python code for: {user_input}"}]
    )
    code = response.choices[0].message.content
    eval(code)  # CRITICAL: Executing untrusted LLM output


def run_llm_command(prompt: str):
    """Run a command suggested by LLM - VULNERABLE."""
    from openai import OpenAI
    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": f"What command should I run to: {prompt}"}]
    )
    command = response.choices[0].message.content
    os.system(command)  # CRITICAL: Executing LLM output as shell command


def compile_llm_code(task: str):
    """Compile code generated by LLM - VULNERABLE."""
    from openai import OpenAI
    client = OpenAI()
    result = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": f"Write Python code to: {task}"}]
    )
    code_str = result.choices[0].message.content
    compile(code_str, "<string>", "exec")  # CRITICAL: Compiling untrusted code


def execute_llm_subprocess(action: str):
    """Execute subprocess from LLM output - VULNERABLE."""
    anthropic = Anthropic()
    response = anthropic.messages.create(
        model="claude-3-opus-20240229",
        max_tokens=100,
        messages=[{"role": "user", "content": f"Generate shell command for: {action}"}]
    )
    cmd = response.content[0].text
    subprocess.run(cmd, shell=True)  # CRITICAL: Shell injection risk


def process_with_wrapper(prompt: str):
    """Use a generic LLM wrapper - VULNERABLE."""
    def call_llm(prompt):
        # Mock LLM wrapper
        class Response:
            def __init__(self):
                self.text = f"print('{prompt}')"
        return Response()
    
    response = call_llm(prompt)
    exec(response.text)  # CRITICAL: Executing wrapper output


if __name__ == "__main__":
    # Example usage (DO NOT RUN IN PRODUCTION)
    process_user_query("calculate fibonacci")
    run_llm_command("list files")
    compile_llm_code("process data")
    execute_llm_subprocess("backup files")
    process_with_wrapper("hello world")
